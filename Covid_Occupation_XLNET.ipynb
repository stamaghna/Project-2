{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Covid_Occupation_XLNET",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS6VEPNX4SkD",
        "outputId": "4f3e2e43-a19a-4400-e865-ed307e03e840"
      },
      "source": [
        "! pip install transformers==3.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.1.95)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noXlYdkP4c2p",
        "outputId": "08a61421-4b19-42c4-e378-ad4470ac3058"
      },
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "\n",
        "#nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "stop_words = stopwords.words('english')\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGcXDqC0xEHn"
      },
      "source": [
        "# try:\n",
        "#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "# except ValueError:\n",
        "#   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "# import os\n",
        "\n",
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# # This is the TPU initialization code that has to be at the beginning.\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "# tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi1l63TT4l8v",
        "outputId": "c14eef6c-0982-434d-9e25-c2b3e2254d67"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD7H7Ezz4pFQ"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Project_2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4WeOjF84sH6"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/Project_2/train_df.csv\")\n",
        "df_ = pd.read_csv(\"/content/drive/My Drive/Project_2/valid_df.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nCu6vkB4uCX"
      },
      "source": [
        "def pp(df1):\n",
        "  #df['mod_text'] = df['text'].replace('\\d+', '')\n",
        "  df1['mod_text'] = df1['text'].apply(lambda x: x[2:-2]) #Removing brackets\n",
        "  df1['mod_text'] = df1['mod_text'].str.replace('#','') #\\S -> Non space character being removed.\n",
        "  df1['mod_text'] = df1['mod_text'].str.replace('@(\\S+) ','') #Removing hastags and username and tags\n",
        "  df1['mod_text'] = df1['mod_text'].str.replace('https:\\S+','') #Removing url\n",
        "  df1['mod_text'] = df1['mod_text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) #Removing next line\n",
        "  df1['mod_text'] = df1['mod_text'].str.replace('[^a-zA-Z0-9]',' ')\n",
        "  #df1['final'] = df1['mod_text'].str.rstrip('\\n')\n",
        "  #df1['final'] = df1['final'].str.replace(' +', '')\n",
        "  #df['mod_text'] = df['mod_text'].apply(lambda x: x[2:-2])\n",
        "  tokenized_doc = df1['mod_text'].apply(lambda x: x.split())\n",
        "  tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "  detokenized_doc = [] \n",
        "  for i in range(len(df1)): \n",
        "    t = ' '.join(tokenized_doc[i]) \n",
        "    detokenized_doc.append(t) \n",
        "  df1['final'] = detokenized_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-zDFNjV4wLD"
      },
      "source": [
        "pp(df)\n",
        "pp(df_)\n",
        "df.final=df.final.apply(str)\n",
        "df_.final=df_.final.apply(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poloOBDy4zMz"
      },
      "source": [
        "df_train, df_val = train_test_split( df , \n",
        "random_state=20, test_size=0.3, stratify=df['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsY2Tc4WGYqu"
      },
      "source": [
        "MAX_LEN = 65"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpzLfQUmFNcq"
      },
      "source": [
        "class Covid_Occup(Dataset):\n",
        "\n",
        "    def __init__(self, texts, targets, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        target = self.targets[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        truncation = True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        input_ids = pad_sequences(encoding['input_ids'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "        input_ids = input_ids.astype(dtype = 'int64')\n",
        "        input_ids = torch.tensor(input_ids) \n",
        "\n",
        "        attention_mask = pad_sequences(encoding['attention_mask'], maxlen=MAX_LEN, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "        attention_mask = attention_mask.astype(dtype = 'int64')\n",
        "        attention_mask = torch.tensor(attention_mask)       \n",
        "\n",
        "        return {\n",
        "        'text_text': text,\n",
        "        'input_ids': input_ids.flatten(),\n",
        "        'attention_mask': attention_mask.flatten(),\n",
        "        'targets': torch.tensor(target, dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZwS2VbMF-v1"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = Covid_Occup(\n",
        "    texts=df.final.to_numpy(),\n",
        "    targets=df.label.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7UxEB0ehl_3"
      },
      "source": [
        "from transformers import XLNetTokenizer, XLNetModel\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'xlnet-base-cased'\n",
        "tokenizer = XLNetTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkTy3LMVGUma"
      },
      "source": [
        "BATCH_SIZE = 6\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "#test_data_loader = create_data_loader(df_, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4lGV2mr5FHX",
        "outputId": "40f0dafb-bf2c-4ebe-882b-e5a53508c0c6"
      },
      "source": [
        "from transformers import XLNetForSequenceClassification\n",
        "#with tpu_strategy.scope(): \n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 2)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScwhbJ_t5Jle"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
        "#optimizer = AdamW(model.parameters(), lr = 3e-4)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dr5jUKkBxVX"
      },
      "source": [
        "from sklearn import metrics\n",
        "def train_epoch(model, data_loader, optimizer, device, #scheduler, \n",
        "                n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    acc = 0\n",
        "    counter = 0\n",
        "  \n",
        "    for d in data_loader:\n",
        "        input_ids = d['input_ids'].to(device)\n",
        "        attention_mask = d['attention_mask'].to(device)\n",
        "        targets = d['targets'].to(device)\n",
        "        \n",
        "        outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # preds = preds.cpu().detach().numpy()\n",
        "        _, prediction = torch.max(outputs[1], dim=1)\n",
        "        targets = targets.cpu().detach().numpy()\n",
        "        prediction = prediction.cpu().detach().numpy()\n",
        "        accuracy = metrics.accuracy_score(targets, prediction)\n",
        "\n",
        "        acc += accuracy\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        counter = counter + 1\n",
        "\n",
        "    return acc / counter, np.mean(losses)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL6Fh-jj5Zmo"
      },
      "source": [
        "def eval_model(model, data_loader, device, n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    acc = 0\n",
        "    counter = 0\n",
        "    #prec = 0\n",
        "\n",
        "    pred = np.array([])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d['input_ids'].to(device)\n",
        "            attention_mask = d['attention_mask'].to(device)\n",
        "            targets = d['targets'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "\n",
        "            _, prediction = torch.max(outputs[1], dim=1)\n",
        "            targets = targets.cpu().detach().numpy()\n",
        "            prediction = prediction.cpu().detach().numpy()\n",
        "            accuracy = metrics.accuracy_score(targets, prediction)\n",
        "            #precision = metrics.precision_score(targets,prediction)\n",
        "            #print(prediction)\n",
        "            pred = np.concatenate((pred, prediction))\n",
        "\n",
        "            acc += accuracy\n",
        "            #prec += precision\n",
        "            losses.append(loss.item())\n",
        "            counter += 1\n",
        "\n",
        "    return acc / counter, np.mean(losses), pred\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "523af5SuDC-a"
      },
      "source": [
        "#len(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drgeLr43Hthk"
      },
      "source": [
        "data = next(iter(train_data_loader))\n",
        "#data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4vlXAhGJCqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4c2e6e-8c3e-4edd-c1a3-533e9ffda64f"
      },
      "source": [
        " input_ids = data['input_ids'].to(device)\n",
        " attention_mask = data['attention_mask'].to(device)\n",
        " targets = data['targets']#.to(device)\n",
        " print(input_ids.shape) # batch size x seq length\n",
        " print(attention_mask.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6, 65])\n",
            "torch.Size([6, 65])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sKrjQhQJK6u"
      },
      "source": [
        "# outputs = model(input_ids.reshape(BATCH_SIZE,MAX_LEN), token_type_ids=None, attention_mask=attention_mask, labels=targets)\n",
        "# #outputs\n",
        "# _, prediction = torch.max(outputs[1], dim=1)\n",
        "# prediction = prediction.cpu().detach().numpy()\n",
        "# print(prediction)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxEjz_WtJUGF"
      },
      "source": [
        "#type(outputs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGpmWVsH5cpi",
        "outputId": "c0bfb32c-3a35-4a7f-ccfd-461b4f8c08e9"
      },
      "source": [
        "#%%time\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(model, train_data_loader, optimizer, device, #scheduler, \n",
        "                                        len(df_train))\n",
        "\n",
        "    print(f'Train loss {train_loss} Train accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss, _ = eval_model(model,val_data_loader, device, len(df_val))\n",
        "\n",
        "    print(f'Val loss {val_loss} Val accuracy {val_acc}')\n",
        "    print()\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    if val_acc > best_accuracy:\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/Project_2/xlnet_model.bin')\n",
        "        best_accuracy = val_acc\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "Train loss 0.4789730848119195 Train accuracy 0.8335714285714277\n",
            "Val loss 0.2915901771001518 Val accuracy 0.925555555555556\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train loss 0.31883435994203735 Train accuracy 0.9304761904761915\n",
            "Val loss 0.26186799774101627 Val accuracy 0.9344444444444452\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train loss 0.2056337396111173 Train accuracy 0.9595238095238103\n",
            "Val loss 0.3486472879024223 Val accuracy 0.9366666666666674\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train loss 0.15149781900682552 Train accuracy 0.9709523809523817\n",
            "Val loss 0.37088444287733485 Val accuracy 0.9322222222222226\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train loss 0.10051445204506827 Train accuracy 0.9819047619047627\n",
            "Val loss 0.4396826852244946 Val accuracy 0.923888888888889\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train loss 0.07317570027026314 Train accuracy 0.9880952380952387\n",
            "Val loss 0.35085222459941484 Val accuracy 0.9466666666666672\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train loss 0.06276898878231545 Train accuracy 0.9900000000000004\n",
            "Val loss 0.38455720194916165 Val accuracy 0.9444444444444451\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train loss 0.04761985583804614 Train accuracy 0.9921428571428574\n",
            "Val loss 0.4962158204555938 Val accuracy 0.9283333333333337\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train loss 0.03243569953431558 Train accuracy 0.9952380952380955\n",
            "Val loss 0.4617792307922112 Val accuracy 0.9372222222222225\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train loss 0.028131819127759496 Train accuracy 0.9961904761904763\n",
            "Val loss 0.4365413995538741 Val accuracy 0.9416666666666672\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj2e7qFr5edM",
        "outputId": "189a9146-fbec-4217-e906-69e86bab77ab"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/My Drive/Project_2/xlnet_model.bin'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJmzYte75gdy"
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzvWxnA2IUcd"
      },
      "source": [
        "# data = next(iter(test_data_loader))\n",
        "# #data.keys()\n",
        "# input_ids = data['input_ids'].to(device)\n",
        "# attention_mask = data['attention_mask'].to(device)\n",
        "# targets = data['targets'].to(device)\n",
        "# print(input_ids.shape)\n",
        "# print(attention_mask.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d57dWifMHrde"
      },
      "source": [
        "# outputs = model(input_ids.reshape(BATCH_SIZE,MAX_LEN), token_type_ids=None, attention_mask=attention_mask, labels=targets)\n",
        "# outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AlAwrtd_HQU",
        "outputId": "9236811b-812b-4308-d5c0-aa33aebf5884"
      },
      "source": [
        "test_data_loader = create_data_loader(df_, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_acc, test_loss, pred = eval_model(model,test_data_loader,device,len(df_))\n",
        "\n",
        "print('Test Accuracy :', test_acc)\n",
        "print('Test Loss :', test_loss)\n",
        "#print('Test Precesion :', test_precision)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy : 0.9361277445109781\n",
            "Test Loss : 0.4096929946908041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KY8gymkQrVc",
        "outputId": "5c2e3578-ee91-4647-b40e-74ff386c67f9"
      },
      "source": [
        "confusion_matrix(df_.label,pred)\n",
        "#pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1463,   60],\n",
              "       [  68,  409]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAyRFwGQbS5a"
      },
      "source": [
        "test = pd.DataFrame()\n",
        "test['index'] = df_['tweet_id']\n",
        "test['XL_Net_label'] = pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j48sRkL-bjIl"
      },
      "source": [
        "from google.colab import files\n",
        "test.to_csv('test_XLNET.csv')\n",
        "#files.download('test_XLNET.csv')\n",
        "!cp test_XLNET.csv \"/content/drive/My Drive/Project_2/test_XLNET.csv\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}